\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{000}
\acmNumber{000}
\acmArticle{000}
\acmYear{2015}
\acmMonth{12}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\doi{0000001.0000001}

%ISSN
\issn{1234-56789}

% Document starts
\begin{document}

\newcommand{\titleName}{General-Purpose GPUs and the Challenges of Efficient
Parallel Processing of Heterogeneous Applications}

% Page heads
\markboth{C. Smith}{\titleName{}}

% Title portion
\title{\titleName{}}
\author{Connor Smith
\affil{Brigham Young University}}

\begin{abstract}
This article is a survey of the current state of general-purpose computing on
GPUs (GPGPUs) and the use of heterogeneous applications on them. The underlying
architecture of GPUs is well-suited to problems with a high level of data
parallelism and has traditionally been used for rendering computer graphics.
Performing general-purpose computing on GPUs is a relatively recent development,
having only become practical and popular in the past decade or so. A brief
overview of a typical GPU architecture is presented, and the challenges of
applying this architecture to heterogeneous problems is discussed. Several
different current areas of research are explored, including warp compression,
dynamic tuning of warps, managing contention between CPUs and GPUs, and compiler
management of heterogeneous applications. While the challenges presented are
difficult to overcome, general-purpose computing on GPUs continues to make
progress in the areas of performance, energy, and power.
\end{abstract}

\keywords{General purpose graphics processing unit, heterogeneous systems, warp
size, dynamic warp, GPU compression, hardware worklists, locality aware mapping}

\acmformat{\titleName{}.}

\begin{bottomstuff}
This work is supported by the National Imaginary Science Foundation, under grant
CNS-0123456.

Author's address: C. Smith, Electrical and Computer Engineering Department,
Brigham Young University.
\end{bottomstuff}

\maketitle


\section{Introduction}
The purpose of this survey is to provide an introduction to general-purpose
computing on graphics processing units (GPGPU), the challenges inherent to this
field, and some active areas of research within the field to alleviate some of
these problems. To the best of the author's knowledge, this is the first survey
in this semester's course to discuss GPGPU.

The term GPU was popularized around the turn of the 21st century by NVIDIA as
consumer-level devices began to need dedicated accelerators to render 3D
graphics. Today, most all computers contain GPUs, from supercomputers to
laptops, desktops to smartphones, although some of these do not use dedicated
graphics cards. The area of GPGPU started to take off roughly a decade ago when it began to be
practical to solve traditional problems with the massively parallel nature of
GPUs. Using hundreds or even thousands of ``cores" to perform computations in
parallel is tempting (even if the cores aren't really cores at all), but brings
with it a new set of challenges.

In a way, the problems of GPGPU are an extension of those sometimes seen in a
traditional desktop multiprocessing system. Even when under a heavy compute
load, it isn't always easy to keep all of the cores doing useful work in any
given cycle. Between limitations in structural units, dependences between
instructions, and latencies of memory accesses, the processor cannot help but
stall sometimes. This problem is exacerbated by orders of magnitude when solving
heterogeneous problems on a GPU. GPUs were originally designed to operate on
graphics processing, problems that are well-structured and have simple, inherent
data parallelism. Problems that do not contain parallelism that is easily mapped
to the architecture of a GPU fail to keep all of the cores in a GPU busy or
strain the memory bandwidth past the GPU's ability to hide latency.

The remainder of this survey is organized as follows. Section
\ref{sec:architecture} provides a brief architectural overview of GPUs. Section
\ref{sec:challenges} elaborates on the challenges specific to GPGPU
applications. Section \ref{sec:research} summarizes and discusses several
current areas of research related to GPGPU. Section \ref{sec:conclusions}
concludes the survey.

% \section{Background}
% \subsection{GPU History}
% TODO
% \subsection{GPGPUs and Current State}
% TODO
% \subsection{Applications}
% TODO

\section{Architectural Overview} \label{sec:architecture}
\subsection{NVidia Warps}
TODO
Multiple threads are grouped together in "warps"
Thread within a warp are mapped to a vector functional unit (SIMD)
NVIDIA's Maxwell GPUs support up to 64 concurrent warps (2048 threads)
Talk about what a streaming multiprocessor is
Modern unified GPUs consist of multiple streaming multiprocessors (SMs)
  - warp (NVIDIA)/wavefronts (OpenCL)
  - similar to a SIMD processor
  - SM has a warp pool to store context of all running threads
  - HW warp scheduler dispatches and allocates resources
  - large (much larger than a CPU) register file shared across all threads
    - Maxwell architecture has 64K 32-bit registers for each SM
  - kernel: large number of threads executing same piece of code
  - multiple threads form a thread-block; same life-cycle, dispatched together
  - threads within a thread-block (32 or 64) organized into warps
  - warp scheduler schedules a ready warp each cycle, hiding latency of stalled warps
  - CPU launches GPU kernels by dispatching launch commands. Kernel parameters are passed from GPU to CPU at launch time and stored in GPU global memory
  - kernels from different streams are independent and can be run concurrently, but kernels within a stream are sequential
  - see DTBL paper for good intro
  - also see Locality Aware paper for internals of a SM
  - cooperative thread array (CTA) - see Weak behavior paper

\section{Challenges} \label{sec:challenges}
\subsection{Heterogeneous Applications}
TODO
Maximizing resource utilization while minimizing shared resource contention between CPU and GPU is hard
\subsection{Memory Limitations}
TODO

\section{New Designs} \label{sec:research}
\begin{itemize}
\item Warp compression stuff
  \begin{itemize}
  \item Assist warps (CABA)
  \item Warped Compression
  \end{itemize}
\item Dynamic tuning
  \begin{itemize}
  \item Coordinated warp scheduling (CAWA)
  \item Dynamic Thread Block Launch (DTBL)
  \item Equalizer
  \item Fine-grain Hardware Worklists (HWWL)
  \item Variable Warp Size (VWS)
  \end{itemize}
\item CPU/GPU contention, NoC
  \begin{itemize}
  \item Managing Concurrency in Heterogeneous
  \end{itemize}
\item Compiler/SW-only design
  \begin{itemize}
  \item Divergence Management
  \item Locality Aware Mapping
  \item (x) WeakBehaviorProgrammingAssumptions
  \end{itemize}
\end{itemize}

\section{Conclusions} \label{sec:conclusions}
\begin{itemize}
\item Mention what the survey talked about
  \begin{itemize}
  \item Background
  \item Overview
  \item Challenges
  \item Designs
  \end{itemize}
\item Explain changes occurring in field
\item Talk about what the future could hold
\end{itemize}

% Acknowledgments
\begin{acks}
TODO The author would like to thank Dr. Penry for teaching this course and
providing extensive design and learning opportunities.
\end{acks}

% % Bibliography
% \bibliographystyle{ACM-Reference-Format-Journals}
% \bibliography{paper-bibfile}

% History dates
\received{December 2015}{December 2015}{December 2015}

\end{document}
