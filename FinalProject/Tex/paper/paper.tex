\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{000}
\acmNumber{000}
\acmArticle{000}
\acmYear{2015}
\acmMonth{12}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\doi{0000001.0000001}

%ISSN
\issn{1234-56789}

% Document starts
\begin{document}

\newcommand{\titleNameLong}{General-Purpose Computing on GPUs and Efficient
Parallel Processing of Heterogeneous Applications}

\newcommand{\titleNameShort}{GPGPU and Efficient Parallel Processing of
Heterogeneous Applications}

% Page heads
\markboth{C. Smith}{\titleNameShort{}}

% Title portion
\title{\titleNameLong{}}
\author{Connor Smith
\affil{Brigham Young University}}

\begin{abstract}
This article is a survey of the current state of general-purpose computing on
GPUs (GPGPUs) and the use of heterogeneous applications on them. The underlying
architecture of GPUs is well-suited to problems with a high level of data
parallelism and has traditionally been used for rendering computer graphics.
Performing general-purpose computing on GPUs is a relatively recent development,
having only become practical and popular in the past decade or so. A brief
overview of a typical GPU architecture is presented, and the challenges of
applying this architecture to heterogeneous problems are discussed. Several
different current areas of research are explored, dynamic warp scheduling and
hardware tuning, reducing memory contention, and software solutions to the
challenges of GPGPU. While the challenges presented are difficult to overcome,
general-purpose computing on GPUs continues to make progress in the areas of
performance, energy, and power.
\end{abstract}

\keywords{General purpose graphics processing unit, heterogeneous systems, warp
size, dynamic warp, GPU compression, hardware worklists, locality aware mapping}

\acmformat{\titleNameLong{}.}

\begin{bottomstuff}
This work is supported by the National Imaginary Science Foundation, under grant
CNS-0123456.

Author's address: C. Smith, Electrical and Computer Engineering Department,
Brigham Young University.
\end{bottomstuff}

\maketitle


\section{Introduction}
The purpose of this survey is to provide an introduction to general-purpose
computing on graphics processing units (GPGPU), the challenges inherent to this
field, and some active areas of research within the field to alleviate some of
these problems. To the best of the author's knowledge, this is the first survey
in this semester's course to discuss GPGPU.

The term GPU was popularized around the turn of the 21st century by NVIDIA as
consumer-level devices began to need dedicated accelerators to render 3D
graphics. Today, most all computers contain GPUs, from supercomputers to
laptops, desktops to smartphones, although some of these do not use dedicated
graphics cards. The area of GPGPU started to take off roughly a decade ago when
it began to be practical to solve traditional problems with the massively
parallel nature of GPUs. Using hundreds or even thousands of ``cores" to perform
computations in parallel is tempting (even if the cores aren't really cores at
all), but brings with it a new set of challenges.

In a way, the problems of GPGPU are an extension of those sometimes seen in a
traditional desktop multiprocessing system. Even when under a heavy compute
load, it isn't always easy to keep all of the cores doing useful work in any
given cycle. Between limitations in structural units, dependences between
instructions, and latencies of memory accesses, the processor cannot help but
stall sometimes. This problem is exacerbated by orders of magnitude when solving
heterogeneous problems on a GPU. GPUs were originally designed to operate on
graphics processing, problems that are well-structured and have simple, inherent
data parallelism. Problems that do not contain parallelism that is easily mapped
to the architecture of a GPU fail to keep all of the cores in a GPU busy or
strain the memory bandwidth past the GPU's ability to hide latency.

The remainder of this survey is organized as follows. Section
\ref{sec:architecture} provides a brief architectural overview of GPUs. Section
\ref{sec:challenges} elaborates on the challenges specific to GPGPU
applications. Section \ref{sec:dynamic} discusses spawning granular helpers to
increase performance. Section \ref{sec:scheduling} deals with more efficient
scheduling within and among streaming multiprocessors. Section \ref{sec:memory}
talks specifically about reducing memory contention. Section \ref{sec:software}
is a discussion of a few software and compiler solutions to the challenges of
GPGPU. Section
\ref{sec:conclusions} concludes the survey.

\section{Architectural Overview} \label{sec:architecture}
\subsection{Streaming Multiprocessors}
Modern unified GPUs consist of multiple streaming multiprocessors (SMs). While
there are many GPU architectures available today, the following description is
based off of NVIDIA's Maxwell architecture, which was released in 2014. These
SMs are similar to SIMD processors and typically contain the following
components:
\begin{description}
  \setlength\itemsep{1em}
  \item[A large register file] The register file is shared across all threads
  and typically contains thousands of registers, many more than are usually
  found in a CPU. NVIDIA's Maxwell architecture has 64K 32-bit registers in each
  SM.
  \item[Multiple caches] Maxwell includes a unified L1/texture cache, which used
  as a coalescing buffer for memory accesses. The buffer gathers requested data
  and sends the entire group to a warp. There are also additional caches TODO.
  \item[Shared memory] There are 96KB available per SM in Maxwell. However,
  thread blocks are limited to 48KB each. Shared memory for a thread-block is
  only accessible by threads within the same thread-block.
  \item[Multiple warp schedulers] Warp schedulers dispatch and allocate
  resources. Maxwell contains four schedulers per SM, and functional units are
  assigned to a particular scheduler. Using a power-of-two number of CUDA cores
  per SM allows for simple partitioning among schedulers. One ready warp is
  scheduled per cycle, which allows the GPU to hide the latency of stalled
  warps.
\end{description}

\subsection{Kernels, Thread-blocks, and Warps}
A kernel is a potentially massive number of threads executing same piece of
code. From the application programmer's point of view, a function is called on
the device with a certain number of blocks and a certain number of threads
within each block. The CPU launches GPU kernels by dispatching launch commands.
Kernel parameters are passed from the CPU to the GPU at launch time and are
stored in the GPU's global memory. Kernels from different streams are
independent and can be run concurrently, but kernels within a stream are
sequential.

Grids are what actually run on the GPU, and they are composed of multiple
``thread- blocks'', also called a cooperative thread array (CTA) outside of
NVIDIA's nomenclature. Maxwell supports up to 64K blocks within a grid. Thread-
blocks contain potentially many threads, on the order of 1024 for modern
architectures. Each thread within a thread-block has the same life-cycle as all
the others, meaning that they are dispatched to and swapped from a SM at the
same time.

Individual threads exist within a thread-block. These threads are grouped into
``warps'', usually around 32 at a time. The warp is the fundamental unit of
execution on GPUs. Threads within a warp are mapped to a vector functional unit
such that all threads execute the same instruction on different data, just a
like a SIMD unit. With this in mind, it would be more accurate to simply call
``threads'' a sequence of SIMD lane instructions, but this survey will continue
to refer to them as threads to maintain consistency with the literature. The
term ``Warp'' is the term used by NVIDIA, while AMD and OpenCL use
``wavefront''. Maxwell supports up to 64 warps resident per SM. Note that this
doesn't mean that all 64 warps are executing at the same time, but that the
architecture can support that many warp contexts concurrently. The SM maintains
a warp pool to store the context of all running threads. Warps are scheduled by
the warp schedulers from this pool.

\section{Challenges of GPGPU} \label{sec:challenges}
\subsection{Heterogeneous Applications}
There are two main challenges of GPGPU. The first is maintaining high
performance on heterogeneous applications. As mentioned previously, this problem
is difficult simply due to the complicated structure of these applications. When
a warp stalls for one reason or another, the warp scheduler selects a new warp
to take its place the next cycle. This is an attempt to hide the latency of warp
stalls. However, GPU schedulers are limited in their ability to hide latencies
for heterogeneous applications. This is caused by imbalanced workloads among
warps, divergent branches, irregular memory accesses, and shared cache
contention \cite{CoordinatedWarpScheduling}. Designs in the following section
discusses these issues and present possible solutions.

\subsection{Memory Limitations}
The second challenge for GPGPU is mitigating memory bandwidth bottlenecks. A
majority of general-purpose applications run on GPUs are memory bound
\cite{AssistWarps}, which means that performance is bottlenecked by off-chip
memory bandwidth. This bottleneck is commonly due to contention among warps
within the GPU, and may also be a result of contention with memory use by the
CPU in heterogeneous system architectures
\cite{ManagingConcurrencyInHeterogeneous}. A number of schemes have been
suggested to reduce pressure on off-chip memory and will be visited in the next
section.

\section{Dynamically-Launched Helpers} \label{sec:dynamic}
\subsection{Assist warps}
As we have seen, GPUs are capable of supporting thousands of threads, but
bottlenecks in heterogeneous applications limit performance. \cite{AssistWarps}
present the idea of using ``assist warps'' in their Core-Assisted Bottleneck
Acceleration (CABA) scheme. These special warps are generated by dedicated
hardware and are used as helper threads to increase performance and efficiency.
Assist warps share the same context as the parent warp and uses some of the
available registers, since investigation shows that 24\% of the register file is
unallocated on average.

One primary use for assist warps is memory compression. Instead of using
dedicated hardware for compressing and decompressing data from memory, this
approach uses existing hardware that is not being utilized. Another advantage of
not using dedicated hardware is the ability to change compression algorithms
flexibly, since different applications have varying access patterns that are
suited for different compression algorithms. Another use for assist warps is in
memoization, in which frequently- computed values from main memory are cached
and used as lookup tables, reducing the number of loads and stores required. One
final use suggested by the authors is using an idle memory pipeline to prefetch
values during computation to reduce the memory pressure.

CABA contains a mix of hardware and software. While software-only or hardware-
only options are possible, the former induces high overhead and complicates
communication between primary and helper warps, and the latter makes having a
flexible interface more difficult. The combination suggested by the authors
dynamically inserts instructions into the execution stream in the form of assist
warps. This requires stores of commonly used assist warps, a controller to
trigger and manage assist warp execution, and an assist warp buffer inside the
instruction buffer that holds triggered warps until they are scheduled. Helper
threads share context with the parent warp, and register values in helper
threads are not preserved between invocations, reducing the overhead of this
technique and does not require any additional resources.

The authors report a reduction in memory bandwidth of 2.1x, resulting in an
increase in performance of 41.7\%. This is only 2.8\% less than the idea case in
which there would be no overhead of assist warps. These helper threads also
reduced the overall system energy by 22.2\%, which is 4.0\% over the ideal
effect. Power consumption is also increased by 2.9\%, but this is a small price
to pay for the performance increases.

\subsection{Dynamic Thread Block Launch}
The authors of Dynamic Thread Block Launch (DTBL) suggest a somewhat similar
approach, this time launching thread blocks dynamically instead of assist warps
\cite{DynamicThreadBlockLaunch}. The motivation behind this technique is that
while emerging general-purpose applications do not have simple, regular
structure that is easy to exploit at the highest level, they still do contain
``pockets'' of parallelism at smaller levels. The authors explain that these
pockets can be exploited with existing architecture using device-side nested
kernel launching, but at a high cost. For example, CUDA supports parent kernels
launching child kernels through device-side API calls. These child kernels can
start at any time after being launched by their parent. This method does exploit
pockets of parallelism, but at a high cost. On average, this CUDA method results
in a 36.1\% decrease in performance, and also consumes significant amounts of
memory by storing pending child kernels.

The problems with existing implementations boil down to four characteristics:
\begin{description}
\item [High kernel density] Kernel launches are expensive, and typical
applications currently require many kernels. For example, a typical breadth-
first search requires about 3,000 device kernel launches, resulting in a high
overhead and memory footprint.
\item [Low compute intensity] While many kernels are launched using these
methods, they are usually fine-grained and do not contain high amounts of
parallelism. The average number of threads in these kernels is around 40, which
is close to the warp size and could be better served by a more fine-grained
approach.
\item [Workload similarity] The operations performed by each dynamically
launched kernel are usually similar, but contain different levels of parallelism
due to differing configurations or data.
\item [Low concurrency/scheduling efficiency] Due to the low compute intensity
of the device kernels, multiple kernels should be able to execute in parallel.
However, GPU architectures place limits on the number of different kernels that
can execute concurrently, and since each kernel has a relatively small number of
warps within it, the SMs are underutilized and struggle to hide memory
latencies.
\end{description}

DTBL is a more fine-grained approach. Instead of launching entire kernels,
individual thread blocks are created and added to the list of existing thread
blocks within a kernel. These thread blocks are coalesced into an aggregated
kernel and executed together. The existing GPU microarchitecture is extended to
support these additional thread blocks by adding data structures to keep track
of dynamically formed groups of thread blocks and associating them with kernels.
Fortunately, this organization is transparent to much of the rest of the
architecture, including the warp schedulers and control divergence mechanism.

The extra data structures of this technique only contribute an area overhead of
0.5\%. Additionally, the authors report a 1.21x performance increase over
original GPU implementations and 1.40x over implementations using existing
device-kernel launches.

\section{Dynamic Scheduling} \label{sec:scheduling}
\subsection{Coordinated warp scheduling (CAWA)}

\subsection{Variable Warp Size (VWS)}
This section left as an exercise to the reader

\subsection{Fine-grain Hardware Worklists (HWWL)}
This section left as an exercise to the reader

\subsection{Equalizer}
This section left as an exercise to the reader

\section{Reducing Memory Contention} \label{sec:memory}
\subsection{Warped Compression}
This section left as an exercise to the reader

\subsection{Managing Concurrency in Heterogeneous (CPU/GPU NoC contention)}
This section left as an exercise to the reader

\section{Compiler/SW-only design} \label{sec:software}
\subsection{Divergence Management}
This section left as an exercise to the reader

\subsection{Locality Aware Mapping}
This section left as an exercise to the reader

\subsection{(x) WeakBehaviorProgrammingAssumptions}
This section left as an exercise to the reader

\section{Conclusions} \label{sec:conclusions}
\begin{itemize}
\item Mention what the survey talked about
  \begin{itemize}
  \item Background
  \item Overview
  \item Challenges
  \item Designs
  \end{itemize}
\item Explain changes occurring in field
\item Talk about what the future could hold
\end{itemize}

% Acknowledgments
\begin{acks}
The author would like to thank Dr. Penry for teaching this course and for
providing extensive design and learning opportunities in this course. It has
been a lot of work, but this course has helped the author know that he has
chosen the right field.
\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{paper-bibfile}

% History dates
\received{December 2015}{December 2015}{December 2015}

\end{document}
