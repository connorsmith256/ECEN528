\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% % Metadata Information
\acmVolume{000}
\acmNumber{000}
\acmArticle{000}
\acmYear{2015}
\acmMonth{12}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\doi{0000001.0000001}

%ISSN
\issn{1234-56789}

% Document starts
\begin{document}

\newcommand{\titleName}{General-Purpose GPUs and the Challenges of Efficient
Parallel Processing of Heterogeneous Applications}

% Page heads
\markboth{C. Smith}{\titleName{}}

% Title portion
\title{\titleName{}}
\author{Connor Smith
\affil{Brigham Young University}}

\begin{abstract}
This article is a survey of the current state of general-purpose computing on
GPUs (GPGPUs) and the use of heterogeneous applications on them. The underlying
architecture of GPUs is well-suited to problems with a high level of data
parallelism and has traditionally been used for rendering computer graphics.
Performing general-purpose computing on GPUs is a relatively recent development,
having only become practical and popular in the past decade or so. A brief
overview of a typical GPU architecture is presented, and the challenges of
applying this architecture to heterogeneous problems is discussed. Several
different current areas of research are explored, including warp compression,
dynamic tuning of warps, managing contention between CPUs and GPUs, and compiler
management of heterogeneous applications. While the challenges presented are
difficult to overcome, general-purpose computing on GPUs continues to make
progress in the areas of performance, energy, and power.
\end{abstract}

\keywords{General purpose graphics processing unit, heterogeneous systems, warp
size, dynamic warp, GPU compression, hardware worklists, locality aware mapping}

\acmformat{\titleName{}.}

\begin{bottomstuff}
This work is supported by the National Imaginary Science Foundation, under grant
CNS-0123456.

Author's address: C. Smith, Electrical and Computer Engineering Department,
Brigham Young University.
\end{bottomstuff}

\maketitle


\section{Introduction}
\begin{itemize}
\item State the purpose of the survey
\item Explain why GPGPU is important and hard
\item Explain what each of the following sections is
\end{itemize}

\section{Background}
\subsection{GPU History}
TODO
\subsection{GPGPUs and Current State}
TODO
\subsection{Applications}
TODO

\section{Architectural Overview}
\subsection{NVidia Warps}
TODO
Multiple threads are grouped together in "warps"
Thread within a warp are mapped to a vector functional unit (SIMD)
NVIDIA's Maxwell GPUs support up to 64 concurrent warps (2048 threads)
Talk about what a streaming multiprocessor is
Modern unified GPUs consist of multiple streaming multiprocessors (SMs)
  - warp (NVIDIA)/wavefronts (OpenCL)
  - similar to a SIMD processor
  - SM has a warp pool to store context of all running threads
  - HW warp scheduler dispatches and allocates resources
  - large (much larger than a CPU) register file shared across all threads
    - Maxwell architecture has 64K 32-bit registers for each SM
  - kernel: large number of threads executing same piece of code
  - multiple threads form a thread-block; same life-cycle, dispatched together
  - threads within a thread-block (32 or 64) organized into warps
  - warp scheduler schedules a ready warp each cycle, hiding latency of stalled warps
  - CPU launches GPU kernels by dispatching launch commands. Kernel parameters are passed from GPU to CPU at launch time and stored in GPU global memory
  - kernels from different streams are independent and can be run concurrently, but kernels within a stream are sequential
  - see DTBL paper for good intro
  - also see Locality Aware paper for internals of a SM
  - cooperative thread array (CTA) - see Weak behavior paper

\section{Challenges}
\subsection{Heterogeneous Applications}
TODO
Maximizing resource utilization while minimizing shared resource contention between CPU and GPU is hard
\subsection{Memory Limitations}
TODO

\section{New Designs}
\begin{itemize}
\item Warp compression stuff
  \begin{itemize}
  \item Assist warps (CABA)
  \item Warped Compression
  \end{itemize}
\item Dynamic tuning
  \begin{itemize}
  \item Coordinated warp scheduling (CAWA)
  \item Dynamic Thread Block Launch (DTBL)
  \item Equalizer
  \item Fine-grain Hardware Worklists (HWWL)
  \item Variable Warp Size (VWS)
  \end{itemize}
\item CPU/GPU contention, NoC
  \begin{itemize}
  \item Managing Concurrency in Heterogeneous
  \end{itemize}
\item Compiler/SW-only design
  \begin{itemize}
  \item Divergence Management
  \item Locality Aware Mapping
  \item (x) WeakBehaviorProgrammingAssumptions
  \end{itemize}
\end{itemize}

\section{Conclusions}
\begin{itemize}
\item Mention what the survey talked about
  \begin{itemize}
  \item Background
  \item Overview
  \item Challenges
  \item Designs
  \end{itemize}
\item Explain changes occurring in field
\item Talk about what the future could hold
\end{itemize}

% Acknowledgments
\begin{acks}
TODO The author would like to thank Dr. Penry for teaching this course and
providing extensive design and learning opportunities.
\end{acks}

% % Bibliography
% \bibliographystyle{ACM-Reference-Format-Journals}
% \bibliography{paper-bibfile}

% History dates
\received{December 2015}{December 2015}{December 2015}

\end{document}
