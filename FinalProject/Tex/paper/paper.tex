\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{000}
\acmNumber{000}
\acmArticle{000}
\acmYear{2015}
\acmMonth{12}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\doi{0000001.0000001}

%ISSN
\issn{1234-56789}

% Document starts
\begin{document}

\newcommand{\titleNameLong}{General-Purpose Computing on GPUs and Efficient
Parallel Processing of Heterogeneous Applications}

\newcommand{\titleNameShort}{GPGPU and Efficient Parallel Processing of
Heterogeneous Applications}

% Page heads
\markboth{C. Smith}{\titleNameShort{}}

% Title portion
\title{\titleNameLong{}}
\author{Connor Smith
\affil{Brigham Young University}}

\begin{abstract}
This article is a survey of the current state of general-purpose computing on
GPUs (GPGPUs) and the use of heterogeneous applications on them. The underlying
architecture of GPUs is well-suited to problems with a high level of data
parallelism and has traditionally been used for rendering computer graphics.
Performing general-purpose computing on GPUs is a relatively recent development,
having only become practical and popular in the past decade or so. A brief
overview of a typical GPU architecture is presented, and the challenges of
applying this architecture to heterogeneous problems are discussed. Several
different current areas of research are explored, dynamic warp scheduling and
hardware tuning, reducing memory contention, and software solutions to the
challenges of GPGPU. While the challenges presented are difficult to overcome,
general-purpose computing on GPUs continues to make progress in the areas of
performance, energy, and power.
\end{abstract}

\keywords{General purpose graphics processing unit, heterogeneous systems, warp
size, dynamic warp, GPU compression, hardware worklists, locality aware mapping}

\acmformat{\titleNameLong{}.}

\begin{bottomstuff}
This work is supported by the National Imaginary Science Foundation, under grant
CNS-0123456.

Author's address: C. Smith, Electrical and Computer Engineering Department,
Brigham Young University.
\end{bottomstuff}

\maketitle


\section{Introduction}
The purpose of this survey is to provide an introduction to general-purpose
computing on graphics processing units (GPGPU), the challenges inherent to this
field, and some active areas of research within the field to alleviate some of
these problems. To the best of the author's knowledge, this is the first survey
in this semester's course to discuss GPGPU.

The term GPU was popularized around the turn of the 21st century by NVIDIA as
consumer-level devices began to need dedicated accelerators to render 3D
graphics. Today, most all computers contain GPUs, from supercomputers to
laptops, desktops to smartphones, although some of these do not use dedicated
graphics cards. The area of GPGPU started to take off roughly a decade ago when
it began to be practical to solve traditional problems with the massively
parallel nature of GPUs. Using hundreds or even thousands of ``cores" to perform
computations in parallel is tempting (even if the cores aren't really cores at
all), but brings with it a new set of challenges.

In a way, the problems of GPGPU are an extension of those sometimes seen in a
traditional desktop multiprocessing system. Even when under a heavy compute
load, it isn't always easy to keep all of the cores doing useful work in any
given cycle. Between limitations in structural units, dependences between
instructions, and latencies of memory accesses, the processor cannot help but
stall sometimes. This problem is exacerbated by orders of magnitude when solving
heterogeneous problems on a GPU. GPUs were originally designed to operate on
graphics processing, problems that are well-structured and have simple, inherent
data parallelism. Problems that do not contain parallelism that is easily mapped
to the architecture of a GPU fail to keep all of the cores in a GPU busy or
strain the memory bandwidth past the GPU's ability to hide latency.

The remainder of this survey is organized as follows. Section
\ref{sec:architecture} provides a brief architectural overview of GPUs. Section
\ref{sec:challenges} elaborates on the challenges specific to GPGPU
applications. Section \ref{sec:dynamic} discusses various dynamic approaches to
scheduling and warp configurations. Section \ref{sec:memory} talks specifically
about reducing memory contention. Section \ref{sec:software} is a discussion of
a few software and compiler solutions to the challenges of GPGPU. Section
\ref{sec:conclusions} concludes the survey.

\section{Architectural Overview} \label{sec:architecture}
\subsection{Streaming Multiprocessors}
Modern unified GPUs consist of multiple streaming multiprocessors (SMs). While
there are many GPU architectures available today, the following description is
based off of NVIDIA's Maxwell architecture, which was released in 2014. These
SMs are similar to SIMD processors and typically contain the following
components:
\begin{description}
  \setlength\itemsep{1em}
  \item[A large register file] The register file is shared across all threads
  and typically contains thousands of registers, many more than are usually
  found in a CPU. NVIDIA's Maxwell architecture has 64K 32-bit registers in each
  SM.
  \item[Multiple caches] Maxwell includes a unified L1/texture cache, which used
  as a coalescing buffer for memory accesses. The buffer gathers requested data
  and sends the entire group to a warp. There are also additional caches TODO.
  \item[Shared memory] There are 96KB available per SM in Maxwell. However,
  thread blocks are limited to 48KB each. Shared memory for a thread-block is
  only accessible by threads within the same thread-block.
  \item[Multiple warp schedulers] Warp schedulers dispatch and allocate
  resources. Maxwell contains four schedulers per SM, and functional units are
  assigned to a particular scheduler. Using a power-of-two number of CUDA cores
  per SM allows for simple partitioning among schedulers. One ready warp is
  scheduled per cycle, which allows the GPU to hide the latency of stalled
  warps.
\end{description}

\subsection{Kernels, Thread-blocks, and Warps}
A kernel is a potentially massive number of threads executing same piece of
code. From the application programmer's point of view, a function is called on
the device with a certain number of blocks and a certain number of threads
within each block. The CPU launches GPU kernels by dispatching launch commands.
Kernel parameters are passed from the CPU to the GPU at launch time and are
stored in the GPU's global memory. Kernels from different streams are
independent and can be run concurrently, but kernels within a stream are
sequential.

Grids are what actually run on the GPU, and they are composed of multiple
``thread- blocks'', also called a cooperative thread array (CTA) outside of
NVIDIA's nomenclature. Maxwell supports up to 64K blocks within a grid. Thread-
blocks contain potentially many threads, on the order of 1024 for modern
architectures. Each thread within a thread-block has the same life-cycle as all
the others, meaning that they are dispatched to and swapped from a SM at the
same time.

Individual threads exist within a thread-block. These threads are grouped into
``warps'', usually around 32 at a time. The warp is the fundamental unit of
execution on GPUs. Threads within a warp are mapped to a vector functional unit
such that all threads execute the same instruction on different data, just a
like a SIMD unit. With this in mind, it would be more accurate to simply call
``threads'' a sequence of SIMD lane instructions, but this survey will continue
to refer to them as threads to maintain consistency with the literature. The
term ``Warp'' is the term used by NVIDIA, while AMD and OpenCL use
``wavefront''. Maxwell supports up to 64 warps resident per SM. Note that this
doesn't mean that all 64 warps are executing at the same time, but that the
architecture can support that many warp contexts concurrently. The SM maintains
a warp pool to store the context of all running threads. Warps are scheduled by
the warp schedulers from this pool.

\section{Challenges of GPGPU} \label{sec:challenges}
\subsection{Heterogeneous Applications}
There are two main challenges of GPGPU. The first is maintaining high
performance on heterogeneous applications. As mentioned previously, this problem
is difficult simply due to the complicated structure of these applications. When
a warp stalls for one reason or another, the warp scheduler selects a new warp
to take its place the next cycle. This is an attempt to hide the latency of warp
stalls. However, GPU schedulers are limited in their ability to hide latencies
for heterogeneous applications. This is caused by imbalanced workloads among
warps, divergent branches, irregular memory accesses, and shared cache
contention \cite{CoordinatedWarpScheduling}. Designs in the following section
discusses these issues and present possible solutions.

\subsection{Memory Limitations}
The second challenge for GPGPU is mitigating memory bandwidth bottlenecks. A
majority of general-purpose applications run on GPUs are memory bound
\cite{AssistWarps}, which means that performance is bottlenecked by off-chip
memory bandwidth. This bottleneck is commonly due to contention among warps
within the GPU, and may also be a result of contention with memory use by the
CPU in heterogeneous system architectures
\cite{ManagingConcurrencyInHeterogeneous}. A number of schemes have been
suggested to reduce pressure on off-chip memory and will be visited in the next
section.

\section{Dynamic Warp Scheduling and Hardware Tuning} \label{sec:dynamic}
\subsection{Assist warps (CABA)}
This section left as an exercise to the reader
\subsection{Coordinated warp scheduling (CAWA)}
This section left as an exercise to the reader
\subsection{Dynamic Thread Block Launch (DTBL)}
This section left as an exercise to the reader
\subsection{Equalizer}
This section left as an exercise to the reader
\subsection{Fine-grain Hardware Worklists (HWWL)}
This section left as an exercise to the reader
\subsection{Variable Warp Size (VWS)}
This section left as an exercise to the reader

\section{Reducing Memory Contention} \label{sec:memory}
\subsection{Warped Compression}
This section left as an exercise to the reader
\subsection{Managing Concurrency in Heterogeneous (CPU/GPU NoC contention)}
This section left as an exercise to the reader

\section{Compiler/SW-only design} \label{sec:software}
\subsection{Divergence Management}
This section left as an exercise to the reader
\subsection{Locality Aware Mapping}
This section left as an exercise to the reader
\subsection{(x) WeakBehaviorProgrammingAssumptions}
This section left as an exercise to the reader

\section{Conclusions} \label{sec:conclusions}
\begin{itemize}
\item Mention what the survey talked about
  \begin{itemize}
  \item Background
  \item Overview
  \item Challenges
  \item Designs
  \end{itemize}
\item Explain changes occurring in field
\item Talk about what the future could hold
\end{itemize}

% Acknowledgments
\begin{acks}
The author would like to thank Dr. Penry for teaching this course and for
providing extensive design and learning opportunities in this course. It has
been a lot of work, but this course has helped the author know that he has
chosen the right field.
\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{paper-bibfile}

% History dates
\received{December 2015}{December 2015}{December 2015}

\end{document}
